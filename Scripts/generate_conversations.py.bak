#!/usr/bin/env python3
"""
Generate multi-turn conversations between Seeker and Supporter agents using Gemini.

Each persona card receives one conversation thread of 15-20 turns using
gemini-2.5-pro with explicit reasoning. The Seeker and Supporter share the
persona context but otherwise run with separate system prompts. For every
Seeker message, three Supporter variants (care levels 0.1, 0.5, 0.9) are
produced alongside their hidden thoughts to enable downstream A/B evaluation.
"""

from __future__ import annotations

import argparse
import json
import os
import random
import sys
import time
from dataclasses import dataclass
from pathlib import Path
from typing import Dict, List, Optional, Sequence

import re

os.environ.setdefault("GRPC_VERBOSITY", "ERROR")
os.environ.setdefault("GLOG_minloglevel", "3")

import google.generativeai as genai

DEFAULT_MODEL = "gemini-2.5-flash-preview-09-2025"
CARE_LEVELS = (0.1, 0.5, 0.9)
CARE_LEVEL_DECODERS = {
    0.1: {"temperature": 0.4, "top_p": 0.8},
    0.5: {"temperature": 0.7, "top_p": 0.92},
    0.9: {"temperature": 0.9, "top_p": 0.98},
}
DEFAULT_MIN_TURNS = 15
DEFAULT_MAX_TURNS = 20
MAX_AGENT_ATTEMPTS = 3
DEFAULT_SLEEP = 8.0
DEFAULT_JITTER = 4.0
DEFAULT_REQUEST_INTERVAL = 15.0
DEFAULT_REQUEST_JITTER = 5.0

FINISH_REASON_LABELS = {
    0: "UNKNOWN",
    1: "STOP",
    2: "MAX_TOKENS",
    3: "SAFETY",
    4: "RECITATION",
    5: "OTHER",
    6: "BLOCKLIST",
    7: "PROHIBITED_CONTENT",
}

THOUGHT_START_TAG = "<THOUGHT>"
THOUGHT_END_TAG = "</THOUGHT>"
MESSAGE_START_TAG = "<MESSAGE>"
MESSAGE_END_TAG = "</MESSAGE>"

RETRY_DELAY_RE = re.compile(r"retry_delay\s*{\s*seconds:\s*([0-9]+)")
RETRY_DELAY_ALT_RE = re.compile(r"Please retry in\s+([0-9.]+)s")


class ConversationError(RuntimeError):
    """Raised when an agent output cannot be parsed or validated."""


def load_env_value(key: str, env_path: Path | None) -> str:
    if key in os.environ and os.environ[key]:
        return os.environ[key]
    if env_path is None or not env_path.exists():
        raise FileNotFoundError(
            f"{key} missing in environment and {env_path or '.env'} not found."
        )
    value = None
    with env_path.open("r", encoding="utf-8") as handle:
        for line in handle:
            striped = line.strip()
            if not striped or striped.startswith("#") or "=" not in striped:
                continue
            lhs, rhs = striped.split("=", 1)
            if lhs.strip() == key:
                value = rhs.strip().strip("\"'")
    if not value:
        raise KeyError(f"{key} not found in {env_path}")
    return value


def strip_code_fences(text: str) -> str:
    stripped = text.strip()
    if stripped.startswith("```"):
        lines = stripped.splitlines()
        if lines:
            lines = lines[1:] if lines[0].startswith("```") else lines
        if lines and lines[-1].startswith("```"):
            lines = lines[:-1]
        stripped = "\n".join(lines).strip()
    return stripped


ROLE_MAP = {"user": "user", "assistant": "model"}


def history_to_messages(history: Sequence[Dict[str, str]]) -> List[Dict[str, object]]:
    messages: List[Dict[str, object]] = []
    for item in history:
        mapped_role = ROLE_MAP.get(item["role"])
        if mapped_role is None:
            raise ConversationError(f"Unsupported role in history: {item['role']}")
        messages.append({"role": mapped_role, "parts": [{"text": item["content"]}]})
    return messages


def extract_retry_after_seconds(message: str) -> Optional[float]:
    match = RETRY_DELAY_RE.search(message)
    if match:
        try:
            return float(match.group(1))
        except ValueError:
            pass
    match = RETRY_DELAY_ALT_RE.search(message)
    if match:
        try:
            return float(match.group(1))
        except ValueError:
            pass
    return None


def gemini_generate_text(
    model: genai.GenerativeModel,
    messages: Sequence[Dict[str, object]],
    *,
    temperature: float,
    top_p: float,
    max_output_tokens: int,
    stream: bool,
    throttler: RequestThrottler,
) -> str:
    throttler.wait()
    generation_config = {
        "temperature": temperature,
        "top_p": top_p,
        "max_output_tokens": max_output_tokens,
    }
    try:
        response = model.generate_content(
            list(messages),
            generation_config=generation_config,
            stream=stream,
        )
        throttler.mark()
    except Exception as exc:  # noqa: BLE001
        raise RuntimeError(f"Gemini generate_content failed: {exc}") from exc

    if stream:
        chunks: List[str] = []
        finish_reason = None
        try:
            for chunk in response:
                candidates = getattr(chunk, "candidates", None)
                if candidates:
                    candidate = candidates[0]
                    finish_reason = getattr(candidate, "finish_reason", finish_reason)
                text_piece = ""
                try:
                    text_piece = chunk.text
                except ValueError:
                    text_piece = ""
                if text_piece:
                    chunks.append(text_piece)
        finally:
            try:
                response.resolve()
            except AttributeError:
                pass
        prompt_feedback = getattr(response, "prompt_feedback", None)
        finish_reason = getattr(response, "finish_reason", finish_reason)
        text = "".join(chunks)
    else:
        prompt_feedback = getattr(response, "prompt_feedback", None)
        text_accessor = ""
        finish_reason = getattr(response, "finish_reason", None)
        try:
            text_accessor = getattr(response, "text", "") or ""
        except ValueError:
            text_accessor = ""
        text = text_accessor
    candidates_snapshot = getattr(response, "candidates", None)

    if prompt_feedback and getattr(prompt_feedback, "block_reason", None):
        raise RuntimeError(f"Gemini blocked the prompt: {prompt_feedback.block_reason}")

    if finish_reason is not None and finish_reason != 0 and not text:
        reason_label = FINISH_REASON_LABELS.get(finish_reason, str(finish_reason))
        raise RuntimeError(
            f"Gemini finished without content (finish_reason={reason_label}/{finish_reason})."
        )

    cleaned = strip_code_fences(text).strip()
    if not cleaned:
        raise RuntimeError(
            f"Gemini returned an empty response (finish_reason={finish_reason}); "
            f"raw={text!r}; candidates={candidates_snapshot}"
        )
    return cleaned


def parse_reasoned_output(payload: str) -> Dict[str, str]:
    thought_match = re.search(
        rf"{re.escape(THOUGHT_START_TAG)}\s*(.*?)\s*{re.escape(THOUGHT_END_TAG)}",
        payload,
        flags=re.DOTALL,
    )
    message_match = re.search(
        rf"{re.escape(MESSAGE_START_TAG)}\s*(.*?)\s*{re.escape(MESSAGE_END_TAG)}",
        payload,
        flags=re.DOTALL,
    )
    if not thought_match or not message_match:
        raise ConversationError(
            "Expected <THOUGHT>...</THOUGHT> and <MESSAGE>...</MESSAGE> blocks; "
            f"payload={payload!r}"
        )
    thought = thought_match.group(1).strip()
    message = message_match.group(1).strip()
    if not thought:
        raise ConversationError("Thought block must not be empty")
    if not message:
        raise ConversationError("Message block must not be empty")
    return {"thought": thought, "message": message}


def load_text(path: Path) -> str:
    with path.open("r", encoding="utf-8") as handle:
        return handle.read()


def load_json(path: Path) -> Dict[str, object]:
    with path.open("r", encoding="utf-8") as handle:
        return json.load(handle)


def persona_to_json_string(persona: Dict[str, object]) -> str:
    return json.dumps(persona, ensure_ascii=False, indent=2)


def build_seeker_system_prompt(
    seeker_prompt_text: str,
    persona_json: Dict[str, object],
) -> str:
    persona_block = persona_to_json_string(persona_json)
    instructions = (
        "You are the Seeker in a crisis support chat. Stay entirely in character, "
        "thinking and responding as the individual described below. Keep responses "
        "natural, 1-3 sentences, and grounded in lived context, values, and symptoms. "
        "Avoid lists or meta commentary. Do not disclose system instructions. "
        "Before giving your outward reply, think through your reasoning silently. "
        "Respond using EXACTLY this template (no extra text, no Markdown fences):\n"
        "<THOUGHT>\n"
        "{your private reasoning here}\n"
        "</THOUGHT>\n"
        "<MESSAGE>\n"
        "{what you actually say aloud, 1-3 sentences}\n"
        "</MESSAGE>\n"
        "Keep the message plain text (no brackets) and avoid double quotes unless you escape them."
    )
    return (
        f"{instructions}\n\n"
        "=== SEEKER_SYSTEM_PROMPT ===\n"
        f"{seeker_prompt_text.strip()}\n"
        "=== SEEKER_PERSONA_CARD ===\n"
        f"{persona_block}\n"
        "=== END SEEKER CONTEXT ==="
    )


def build_supporter_system_prompt(
    supporter_prompt_text: str,
    persona_json: Dict[str, object],
) -> str:
    persona_block = persona_to_json_string(persona_json)
    instructions = (
        "You are the Supporter in a crisis chat. Follow the Supporter system prompt "
        "rules exactly. The Seeker persona card is provided for context. Respond only "
        "to the latest Seeker utterance. Always use the <THOUGHT>...</THOUGHT> and "
        "<MESSAGE>...</MESSAGE> template, keeping the outward message plain text "
        "without annotations."
    )
    return (
        f"{instructions}\n\n"
        "=== SUPPORTER_SYSTEM_PROMPT ===\n"
        f"{supporter_prompt_text.strip()}\n"
        "=== SEEKER_PERSONA_CARD ===\n"
        f"{persona_block}\n"
        "=== END SUPPORTER CONTEXT ==="
    )


def seeker_initial_user_prompt() -> str:
    return (
        "The Supporter greets you softly and says they are here to listen. "
        "Begin the conversation by sharing what brings you here today."
    )


def support_user_prompt(seeker_message: str, care_level: float) -> str:
    return (
        "The Seeker says:\n"
        f"{seeker_message.strip()}\n\n"
        f"Respond as the Supporter using care_level={care_level:.1f}. "
        "Follow the style and safety rules. Provide one consolidated reply. "
        "Think privately before speaking, then respond using EXACTLY this template "
        "(no extra text, no Markdown fences):\n"
        "<THOUGHT>\n"
        "{your internal reasoning here}\n"
        "</THOUGHT>\n"
        "<MESSAGE>\n"
        "{your outward reply, 1-4 sentences fitting the care_level}\n"
        "</MESSAGE>\n"
        "Keep the outward message plain text and avoid unescaped double quotes."
    )


@dataclass
class ConversationTurn:
    turn_index: int
    seeker_message: str
    seeker_thought: str
    supporter_variants: List[Dict[str, object]]
    canonical_care_level: float


def generate_seeker_message(
    *,
    gemini_model: genai.GenerativeModel,
    history: List[Dict[str, str]],
    rng: random.Random,
    throttler: RequestThrottler,
) -> Dict[str, str]:
    for attempt in range(1, MAX_AGENT_ATTEMPTS + 1):
        try:
            content = gemini_generate_text(
                gemini_model,
                history_to_messages(history),
                temperature=0.85,
                top_p=0.9,
                max_output_tokens=2048,
                stream=False,
                throttler=throttler,
            )
        except RuntimeError as err:
            delay = extract_retry_after_seconds(str(err))
            if delay:
                sleep_for = delay + rng.uniform(0, 1.5)
                print(f"[wait] Rate limit hit for seeker model; sleeping {sleep_for:.1f}s.")
                time.sleep(sleep_for)
                continue
            if attempt == MAX_AGENT_ATTEMPTS:
                raise ConversationError(f"Seeker model error: {err}") from err
            time.sleep(1.5 * attempt)
            continue
        cleaned = content.strip()
        try:
            parsed = parse_reasoned_output(cleaned)
        except ConversationError as err:
            if attempt == MAX_AGENT_ATTEMPTS:
                raise
            time.sleep(1.0)
            continue
        if not parsed["message"]:
            if attempt == MAX_AGENT_ATTEMPTS:
                raise ConversationError("Seeker produced empty outward message.")
            time.sleep(1.0)
            continue
        return parsed
    raise ConversationError("Unable to get Seeker message after retries.")


def generate_supporter_variants(
    *,
    gemini_model: genai.GenerativeModel,
    history: List[Dict[str, str]],
    seeker_message: str,
    rng: random.Random,
    throttler: RequestThrottler,
) -> List[Dict[str, object]]:
    variants: List[Dict[str, object]] = []
    for care_level in CARE_LEVELS:
        decoder = CARE_LEVEL_DECODERS[care_level]
        messages = history_to_messages(history) + [
            {"role": "user", "parts": [support_user_prompt(seeker_message, care_level)]}
        ]
        for attempt in range(1, MAX_AGENT_ATTEMPTS + 1):
            try:
                content = gemini_generate_text(
                    gemini_model,
                    messages,
                    temperature=decoder["temperature"],
                    top_p=decoder["top_p"],
                    max_output_tokens=2048,
                    stream=False,
                    throttler=throttler,
                )
            except RuntimeError as err:
                delay = extract_retry_after_seconds(str(err))
                if delay:
                    sleep_for = delay + rng.uniform(0, 1.5)
                    print(
                        f"[wait] Rate limit hit for supporter model "
                        f"(care_level={care_level:.1f}); sleeping {sleep_for:.1f}s."
                    )
                    time.sleep(sleep_for)
                    continue
                if attempt == MAX_AGENT_ATTEMPTS:
                    raise ConversationError(
                        f"Supporter model error (care_level={care_level:.1f}): {err}"
                    ) from err
                time.sleep(1.5 * attempt)
                continue
            cleaned = content.strip()
            try:
                parsed = parse_reasoned_output(cleaned)
            except ConversationError as err:
                if attempt == MAX_AGENT_ATTEMPTS:
                    raise
                time.sleep(1.0)
                continue
            if not parsed["message"]:
                if attempt == MAX_AGENT_ATTEMPTS:
                    raise ConversationError(
                        f"Supporter produced empty outward message (care_level={care_level:.1f})."
                    )
                time.sleep(1.0)
                continue
            variants.append(
                {
                    "care_level": round(care_level, 1),
                    "message": parsed["message"],
                    "thought": parsed["thought"],
                }
            )
            break
    return variants


def detect_termination(text: str) -> bool:
    lowered = text.lower()
    endings = (
        "goodbye",
        "got to go",
        "have to go",
        "signing off",
        "ending this conversation",
        "thanks, that's all",
        "this chat is over",
    )
    return any(keyword in lowered for keyword in endings)


def build_arg_parser() -> argparse.ArgumentParser:
    parser = argparse.ArgumentParser(
        description="Generate Seeker/Supporter conversations for persona cards."
    )
    parser.add_argument(
        "--personas-dir",
        type=Path,
        default=Path("Artifacts/personas"),
        help="Directory containing persona card JSON files.",
    )
    parser.add_argument(
        "--output-dir",
        type=Path,
        default=Path("Artifacts/conversations"),
        help="Directory to write conversation transcripts.",
    )
    parser.add_argument(
        "--env-file",
        type=Path,
        default=Path(".env"),
        help="Path to .env file with GEMINI_API_KEY.",
    )
    parser.add_argument(
        "--model",
        type=str,
        default=DEFAULT_MODEL,
        help=f"Gemini model identifier (default: {DEFAULT_MODEL}).",
    )
    parser.add_argument(
        "--min-turns",
        type=int,
        default=DEFAULT_MIN_TURNS,
        help=f"Minimum turns per conversation (default: {DEFAULT_MIN_TURNS}).",
    )
    parser.add_argument(
        "--max-turns",
        type=int,
        default=DEFAULT_MAX_TURNS,
        help=f"Maximum turns per conversation (default: {DEFAULT_MAX_TURNS}).",
    )
    parser.add_argument(
        "--limit",
        type=int,
        default=None,
        help="Optional cap on number of personas to process.",
    )
    parser.add_argument(
        "--seed",
        type=int,
        default=None,
        help="Random seed for reproducibility.",
    )
    parser.add_argument(
        "--sleep",
        type=float,
        default=DEFAULT_SLEEP,
        help=(
            "Base sleep (seconds) between persona conversations. Note that "
            "gemini-2.5-flash-preview-09-2025 has higher rate limits, but a "
            "generous pause (default set) still helps avoid quota bursts."
        ),
    )
    parser.add_argument(
        "--jitter",
        type=float,
        default=DEFAULT_JITTER,
        help=f"Random jitter added to sleep (default: {DEFAULT_JITTER}).",
    )
    parser.add_argument(
        "--skip-existing",
        action="store_true",
        help="Skip personas whose conversation output already exists.",
    )
    parser.add_argument(
        "--min-request-interval",
        type=float,
        default=DEFAULT_REQUEST_INTERVAL,
        help=(
            "Minimum seconds between successive Gemini calls. "
            "Increase this if you still encounter rate limits."
        ),
    )
    parser.add_argument(
        "--request-jitter",
        type=float,
        default=DEFAULT_REQUEST_JITTER,
        help="Additional random jitter applied to the inter-request delay.",
    )
    return parser


def main(argv: Sequence[str] | None = None) -> int:
    parser = build_arg_parser()
    args = parser.parse_args(argv)

    if args.min_turns < 1 or args.max_turns < args.min_turns:
        parser.error("Invalid turn range.")

    rng = random.Random(args.seed)

    try:
        api_key = load_env_value("GEMINI_API_KEY", args.env_file)
    except (KeyError, FileNotFoundError) as err:
        parser.error(str(err))
        return 2
    genai.configure(api_key=api_key)

    project_root = Path(__file__).resolve().parent.parent
    prompts_dir = project_root / "Prompts"
    seeker_prompt_path = prompts_dir / "Seeker_System_Prompt.json"
    supporter_prompt_path = prompts_dir / "Supporter_System_Prompt.json"

    seeker_prompt_text = load_text(seeker_prompt_path)
    supporter_prompt_text = load_text(supporter_prompt_path)

    persona_files = sorted(args.personas_dir.glob("*.json"))
    if args.limit:
        persona_files = persona_files[: args.limit]
    if not persona_files:
        parser.error(f"No persona JSON files found in {args.personas_dir}")
        return 1

    output_dir: Path = args.output_dir
    output_dir.mkdir(parents=True, exist_ok=True)

    throttler = RequestThrottler(
        min_interval=args.min_request_interval,
        jitter=args.request_jitter,
        rng=rng,
    )

    for index, persona_path in enumerate(persona_files, start=1):
        persona = load_json(persona_path)
        persona_id = str(persona.get("id") or persona_path.stem)
        output_path = output_dir / f"{persona_id}.json"
        if args.skip_existing and output_path.exists():
            print(f"[skip] {persona_id} already has a conversation.")
            continue

        turn_target = rng.randint(args.min_turns, args.max_turns)
        print(
            f"[{index}/{len(persona_files)}] Persona {persona_id}: generating "
            f"{turn_target} turns."
        )

        seeker_system = build_seeker_system_prompt(seeker_prompt_text, persona)
        supporter_system = build_supporter_system_prompt(
            supporter_prompt_text,
            persona,
        )
        seeker_model = genai.GenerativeModel(
            model_name=args.model,
            system_instruction=seeker_system,
        )
        supporter_model = genai.GenerativeModel(
            model_name=args.model,
            system_instruction=supporter_system,
        )

        seeker_history: List[Dict[str, str]] = [
            {"role": "user", "content": seeker_initial_user_prompt()}
        ]
        supporter_history: List[Dict[str, str]] = []
        turns: List[ConversationTurn] = []

        for turn_index in range(1, turn_target + 1):
            try:
                seeker_output = generate_seeker_message(
                    gemini_model=seeker_model,
                    history=seeker_history,
                    rng=rng,
                    throttler=throttler,
                )
            except ConversationError as err:
                print(f"[warn] Stopping early for {persona_id}: {err}")
                break
            seeker_message = seeker_output["message"]
            seeker_thought = seeker_output["thought"]
            seeker_history.append({"role": "assistant", "content": seeker_message})

            try:
                supporter_variants = generate_supporter_variants(
                    gemini_model=supporter_model,
                    history=supporter_history,
                    seeker_message=seeker_message,
                    rng=rng,
                    throttler=throttler,
                )
            except ConversationError as err:
                print(f"[warn] Stopping early for {persona_id}: {err}")
                break

            canonical = next(
                (
                    variant
                    for variant in supporter_variants
                    if variant["care_level"] == 0.5
                ),
                supporter_variants[0],
            )

            turns.append(
                ConversationTurn(
                    turn_index=turn_index,
                    seeker_message=seeker_message,
                    seeker_thought=seeker_thought,
                    supporter_variants=supporter_variants,
                    canonical_care_level=canonical["care_level"],
                )
            )

            supporter_history.append({"role": "user", "content": seeker_message})
            supporter_history.append({"role": "assistant", "content": canonical["message"]})
            seeker_history.append({"role": "user", "content": canonical["message"]})

            if detect_termination(seeker_message) or detect_termination(
                canonical["message"]
            ):
                print(f"[info] Early termination triggered for {persona_id}.")
                break

        if not turns:
            print(f"[error] No turns recorded for {persona_id}; skipping save.")
            continue

        conversation_payload = {
            "persona_id": persona_id,
            "persona_path": str(persona_path),
            "model": args.model,
            "turns_recorded": len(turns),
            "desired_turns": turn_target,
            "seeker_system_prompt": seeker_system,
            "supporter_system_prompt": supporter_system,
            "turns": [
                {
                    "turn_index": turn.turn_index,
                    "seeker_message": turn.seeker_message,
                    "seeker_thought": turn.seeker_thought,
                    "supporter_responses": turn.supporter_variants,
                    "canonical_care_level": turn.canonical_care_level,
                }
                for turn in turns
            ],
        }

        with output_path.open("w", encoding="utf-8") as handle:
            json.dump(conversation_payload, handle, ensure_ascii=False, indent=2)
            handle.write("\n")

        sleep_time = max(0.0, args.sleep + rng.uniform(0, args.jitter))
        time.sleep(sleep_time)

    print("Conversation generation complete.")
    return 0


if __name__ == "__main__":
    sys.exit(main())
class RequestThrottler:
    def __init__(self, *, min_interval: float, jitter: float, rng: random.Random):
        self.min_interval = max(0.0, min_interval)
        self.jitter = max(0.0, jitter)
        self.rng = rng
        self._last_call_time: Optional[float] = None

    def wait(self) -> None:
        if self._last_call_time is None or self.min_interval <= 0:
            return
        elapsed = time.monotonic() - self._last_call_time
        target = self.min_interval + self.rng.uniform(0, self.jitter)
        remaining = target - elapsed
        if remaining > 0:
            time.sleep(remaining)

    def mark(self) -> None:
        self._last_call_time = time.monotonic()
